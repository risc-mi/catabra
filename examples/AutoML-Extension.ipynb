{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273f0434",
   "metadata": {},
   "source": [
    "# Add New AutoML Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71e4f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92037b12",
   "metadata": {},
   "source": [
    "This notebook is part of the [CaTabRa GitHub repository](https://github.com/risc-mi/catabra)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22af802",
   "metadata": {},
   "source": [
    "This short example demonstrates how a new AutoML backend can be added to CaTabRa, i.e.,\n",
    "\n",
    "* [how it can be implemented](#Implement-Random-Search), and\n",
    "* [how it can be utilized in CaTabRa's data analysis workflow](#Utilize-Random-Search).\n",
    "\n",
    "It also briefly explains [how the existing auto-sklearn backend can be extended](#Extend-Existing-Auto-Sklearn-Backend) without having to add new backend from scratch.\n",
    "\n",
    "For the related question of how to conveniently utilize a fixed ML pipeline (without hyperparameter optimization) refer to [this example](https://catabra.readthedocs.io/en/latest/jupyter/fixed_pipeline.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c5d2c",
   "metadata": {},
   "source": [
    "## Implement Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45faf8",
   "metadata": {},
   "source": [
    "We implement a simple random search over a fixed, non-configurable parameter grid.\n",
    "\n",
    "**ATTENTION!** This is an extremely reduced example that only serves demonstration purposes.\n",
    "It lacks many capabilities normally expected from CaTabRa AutoML backends, like\n",
    "\n",
    "* supporting different prediction tasks (not just binary- and multiclass classification),\n",
    "* handling numerical and categorical features,\n",
    "* handling unlabeled samples,\n",
    "* supporting grouped splitting for internal validation,\n",
    "* taking time- and memory constraints into accoount,\n",
    "* taking different optimization objectives into account,\n",
    "* logging the training process,\n",
    "* building ensembles,\n",
    "* etc.\n",
    "\n",
    "If you intend to actually add a new AutoML backend, have a look at the implementation of the default auto-sklearn backend in [`catabra.automl.askl.backend`](https://github.com/risc-mi/catabra/tree/main/catabra/automl/askl/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754b48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from catabra.automl.base import FittedEnsemble, AutoMLBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbadf1",
   "metadata": {},
   "source": [
    "AutoML backends need to implement the abstract base class [`catabra.automl.base.AutoMLBackend`](https://github.com/risc-mi/catabra/tree/main/catabra/automl/base.py). The main methods of interest are `fit()`, `predict()` and `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d5203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearchBackend(AutoMLBackend):\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return 'random_search'\n",
    "    \n",
    "    @property\n",
    "    def model_ids_(self) -> list:\n",
    "        [0]\n",
    "    \n",
    "    def summary(self) -> dict:\n",
    "        return {0: [' '.join(repr(s[1]).replace('\\n', ' ').split()) for s in self.random_search_.best_estimator_.steps]}\n",
    "    \n",
    "    def training_history(self) -> pd.DataFrame:\n",
    "        hist = pd.DataFrame(self.random_search_.cv_results_)\n",
    "        hist.rename({'mean_test_score': 'val_score'}, axis=1, inplace=True)   # for plotting\n",
    "        return hist\n",
    "    \n",
    "    def fitted_ensemble(self, ensemble_only: bool = True) -> FittedEnsemble:\n",
    "        pip = self.random_search_.best_estimator_\n",
    "        return FittedEnsemble(\n",
    "            task=self.task,\n",
    "            models={\n",
    "                0: dict(preprocessing=pip.steps[0][1], estimator=pip.steps[1][1])\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def fit(self, x_train: pd.DataFrame, y_train: pd.DataFrame, groups: Optional[np.ndarray] = None,\n",
    "            sample_weights: Optional[np.ndarray] = None, time: Optional[int] = None, jobs: Optional[int] = None,\n",
    "            dataset_name: Optional[str] = None, monitor=None) -> 'RandomSearchBackend':\n",
    "        \n",
    "        assert self.task in ('binary_classification', 'multiclass_classification')\n",
    "        assert y_train.notna().all().all()\n",
    "        assert groups is None\n",
    "        assert sample_weights is None\n",
    "        \n",
    "        metrics = self.config.get(self.task + '_metrics', [])\n",
    "        assert len(metrics) == 0 or metrics[0] == 'accuracy'\n",
    "        \n",
    "        pip = Pipeline(\n",
    "            [\n",
    "                ('imputer', SimpleImputer()),\n",
    "                ('classifier', RandomForestClassifier())\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        param_dist = {\n",
    "            'imputer__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "            'imputer__add_indicator': [True, False],\n",
    "            'classifier__n_estimators': [10, 20, 50, 80, 100, 150, 200],\n",
    "            'classifier__criterion': ['gini', 'entropy'],\n",
    "            'classifier__max_depth': [None, 4, 10],\n",
    "            'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        }\n",
    "        \n",
    "        if time is None:\n",
    "            time = 1\n",
    "        \n",
    "        # abuse `time` as number of iterations\n",
    "        n_iter = time\n",
    "        \n",
    "        self.random_search_ = RandomizedSearchCV(pip, param_distributions=param_dist, n_iter=n_iter, refit=True)\n",
    "        self.random_search_.fit(x_train.values, y_train.values[:, 0])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x: pd.DataFrame, jobs: Optional[int] = None, batch_size: Optional[int] = None,\n",
    "                model_id=None, calibrated: bool = 'auto') -> np.ndarray:\n",
    "        return self.random_search_.predict(x)\n",
    "    \n",
    "    def predict_proba(self, x: pd.DataFrame, jobs: Optional[int] = None, batch_size: Optional[int] = None,\n",
    "                      model_id=None, calibrated: bool = 'auto') -> np.ndarray:\n",
    "        return self.random_search_.predict_proba(x)\n",
    "    \n",
    "    def predict_all(self, x: pd.DataFrame, jobs: Optional[int] = None, batch_size: Optional[int] = None) -> dict:\n",
    "        return {0: self.predict(x, jobs=jobs, batch_size=batch_size)}\n",
    "    \n",
    "    def predict_proba_all(self, x: pd.DataFrame, jobs: Optional[int] = None, batch_size: Optional[int] = None) -> dict:\n",
    "        return {0: self.predict_proba(x, jobs=jobs, batch_size=batch_size)}\n",
    "    \n",
    "    def get_versions(self) -> dict:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d90d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoMLBackend.register('random_search', RandomSearchBackend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61c7e8",
   "metadata": {},
   "source": [
    "## Utilize Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65cc6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa3d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target labels to DataFrame\n",
    "X['diagnosis'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430dcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train- and test set by adding column with corresponding values\n",
    "# the name of the column is arbitrary; CaTabRa tries to \"guess\" which samples belong to which set based on the column name and -values\n",
    "X['train'] = X.index <= 0.8 * len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3702f",
   "metadata": {},
   "source": [
    "When analyzing the data, we inform CaTabRa that we want to use the `\"random_search\"` backend by adjusting the config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ab35bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaTabRa] ### Analysis started at 2023-02-09 09:30:27.137817\n",
      "[CaTabRa] Saving descriptive statistics completed\n",
      "[CaTabRa] Using AutoML-backend random_search for binary_classification\n",
      "[CaTabRa] Final training statistics:\n",
      "    n_models_trained: 20\n",
      "[CaTabRa] Creating shap explainer\n",
      "[CaTabRa] Initialized out-of-distribution detector of type Autoencoder\n",
      "[CaTabRa] Fitting out-of-distribution detector...\n",
      "Iteration 1, loss = 0.06783438\n",
      "Iteration 2, loss = 0.03997528\n",
      "Iteration 3, loss = 0.02633058\n",
      "Iteration 4, loss = 0.01948884\n",
      "Iteration 5, loss = 0.01487442\n",
      "Iteration 6, loss = 0.01228704\n",
      "Iteration 7, loss = 0.01144362\n",
      "Iteration 8, loss = 0.01063012\n",
      "Iteration 9, loss = 0.00981005\n",
      "Iteration 10, loss = 0.00913160\n",
      "Iteration 11, loss = 0.00833614\n",
      "Iteration 12, loss = 0.00764720\n",
      "Iteration 13, loss = 0.00714880\n",
      "Iteration 14, loss = 0.00660951\n",
      "Iteration 15, loss = 0.00632128\n",
      "Iteration 16, loss = 0.00613749\n",
      "Iteration 17, loss = 0.00583286\n",
      "Iteration 18, loss = 0.00577213\n",
      "Iteration 19, loss = 0.00582528\n",
      "Iteration 20, loss = 0.00698503\n",
      "Iteration 21, loss = 0.00653891\n",
      "Iteration 22, loss = 0.00593587\n",
      "Iteration 23, loss = 0.00592284\n",
      "Iteration 24, loss = 0.00581431\n",
      "Iteration 25, loss = 0.00573134\n",
      "Iteration 26, loss = 0.00559525\n",
      "Iteration 27, loss = 0.00543705\n",
      "Iteration 28, loss = 0.00539677\n",
      "Iteration 29, loss = 0.00539616\n",
      "Iteration 30, loss = 0.00539520\n",
      "Iteration 31, loss = 0.00533531\n",
      "Iteration 32, loss = 0.00531575\n",
      "Iteration 33, loss = 0.00529221\n",
      "Iteration 34, loss = 0.00524817\n",
      "Iteration 35, loss = 0.00523697\n",
      "Iteration 36, loss = 0.00521718\n",
      "Iteration 37, loss = 0.00521684\n",
      "Iteration 38, loss = 0.00520618\n",
      "Iteration 39, loss = 0.00520097\n",
      "Iteration 40, loss = 0.00520765\n",
      "Iteration 41, loss = 0.00520148\n",
      "Iteration 42, loss = 0.00519837\n",
      "Iteration 43, loss = 0.00518533\n",
      "Iteration 44, loss = 0.00518255\n",
      "Iteration 45, loss = 0.00518003\n",
      "Iteration 46, loss = 0.00517438\n",
      "Iteration 47, loss = 0.00517886\n",
      "Iteration 48, loss = 0.00518837\n",
      "Iteration 49, loss = 0.00516961\n",
      "Iteration 50, loss = 0.00519560\n",
      "Iteration 51, loss = 0.00516057\n",
      "Iteration 52, loss = 0.00517097\n",
      "Iteration 53, loss = 0.00515444\n",
      "Iteration 54, loss = 0.00515273\n",
      "Iteration 55, loss = 0.00514750\n",
      "Iteration 56, loss = 0.00514033\n",
      "Iteration 57, loss = 0.00514505\n",
      "Iteration 58, loss = 0.00514759\n",
      "Iteration 59, loss = 0.00515513\n",
      "Iteration 60, loss = 0.00514548\n",
      "Iteration 61, loss = 0.00513684\n",
      "Iteration 62, loss = 0.00512496\n",
      "Iteration 63, loss = 0.00513075\n",
      "Iteration 64, loss = 0.00512894\n",
      "Iteration 65, loss = 0.00511904\n",
      "Iteration 66, loss = 0.00512427\n",
      "Iteration 67, loss = 0.00512083\n",
      "Iteration 68, loss = 0.00512339\n",
      "Iteration 69, loss = 0.00511534\n",
      "Iteration 70, loss = 0.00510811\n",
      "Iteration 71, loss = 0.00514462\n",
      "Iteration 72, loss = 0.00512639\n",
      "Iteration 73, loss = 0.00512267\n",
      "Iteration 74, loss = 0.00514646\n",
      "Iteration 75, loss = 0.00511554\n",
      "Iteration 76, loss = 0.00511430\n",
      "Iteration 77, loss = 0.00511921\n",
      "Iteration 78, loss = 0.00511720\n",
      "Training loss did not improve more than tol=0.000100 for 50 consecutive epochs. Stopping.\n",
      "[CaTabRa] Out-of-distribution detector fitted.\n",
      "[CaTabRa] ### Analysis finished at 2023-02-09 09:30:41.778586\n",
      "[CaTabRa] ### Elapsed time: 0 days 00:00:14.640769\n",
      "[CaTabRa] ### Output saved in /mnt/c/Users/amaletzk/Documents/CaTabRa/catabra/examples/random_search_example\n",
      "[CaTabRa] ### Evaluation started at 2023-02-09 09:30:41.827327\n",
      "[CaTabRa] Predicting out-of-distribution samples.\n",
      "[CaTabRa] Saving descriptive statistics completed\n",
      "[CaTabRa] Saving descriptive statistics completed\n",
      "[CaTabRa] Evaluation results for train:\n",
      "    accuracy @ 0.5: 0.9978070175438597\n",
      "    roc_auc: 0.9999999999999999\n",
      "[CaTabRa] Evaluation results for not_train:\n",
      "    accuracy @ 0.5: 0.9734513274336283\n",
      "    roc_auc: 0.9982316534040672\n",
      "[CaTabRa] ### Evaluation finished at 2023-02-09 09:30:46.508033\n",
      "[CaTabRa] ### Elapsed time: 0 days 00:00:04.680706\n",
      "[CaTabRa] ### Output saved in /mnt/c/Users/amaletzk/Documents/CaTabRa/catabra/examples/random_search_example/eval\n"
     ]
    }
   ],
   "source": [
    "from catabra.analysis import analyze\n",
    "\n",
    "analyze(\n",
    "    X,\n",
    "    classify='diagnosis',     # name of column containing classification target\n",
    "    split='train',            # name of column containing information about the train-test split (optional)\n",
    "    time=20,                  # ONLY IN THIS CASE: number of random search iterations\n",
    "    out='random_search_example',\n",
    "    config={\n",
    "        'automl': 'random_search',     # name of the AutoML backend\n",
    "        'binary_classification_metrics': ['accuracy', 'roc_auc'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c20f4f",
   "metadata": {},
   "source": [
    "After implementing the (simplistic) new AutoML backend in a few lines of code, CaTabRa takes care of everything else: calculating descriptive statistics, splitting the data into training- and a test sets, training a classifier and an OOD detector, and evaluating the classifier on both training- and test set (including visualizations).\n",
    "\n",
    "We can inspect the training history and the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac2834d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catabra.util import io\n",
    "training_history = io.read_df('random_search_example/training_history.xlsx')\n",
    "model_summary = io.load('random_search_example/model_summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4756e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "      <th>param_imputer__add_indicator</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__criterion</th>\n",
       "      <th>param_classifier__class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.102886</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'imputer__strategy': 'median', 'imputer__add_...</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964955</td>\n",
       "      <td>0.018782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>entropy</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>{'imputer__strategy': 'median', 'imputer__add_...</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.962757</td>\n",
       "      <td>0.011020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.082439</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gini</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'imputer__strategy': 'most_frequent', 'impute...</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.960559</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>gini</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>{'imputer__strategy': 'median', 'imputer__add_...</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.960535</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.082382</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>entropy</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'imputer__strategy': 'median', 'imputer__add_...</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.958409</td>\n",
       "      <td>0.018596</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10       0.102886      0.002096         0.007032        0.000338   \n",
       "18       0.124079      0.002483         0.006806        0.000152   \n",
       "5        0.082439      0.000397         0.005633        0.000063   \n",
       "14       0.025500      0.000564         0.001812        0.000055   \n",
       "9        0.082382      0.001896         0.005584        0.000116   \n",
       "\n",
       "   param_imputer__strategy  param_imputer__add_indicator  \\\n",
       "10                  median                         False   \n",
       "18                  median                         False   \n",
       "5            most_frequent                         False   \n",
       "14                  median                          True   \n",
       "9                   median                          True   \n",
       "\n",
       "    param_classifier__n_estimators  param_classifier__max_depth  \\\n",
       "10                             100                          NaN   \n",
       "18                             100                          NaN   \n",
       "5                               80                          NaN   \n",
       "14                              20                         10.0   \n",
       "9                               80                          NaN   \n",
       "\n",
       "   param_classifier__criterion param_classifier__class_weight  \\\n",
       "10                     entropy                            NaN   \n",
       "18                     entropy             balanced_subsample   \n",
       "5                         gini                       balanced   \n",
       "14                        gini             balanced_subsample   \n",
       "9                      entropy                       balanced   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "10  {'imputer__strategy': 'median', 'imputer__add_...           0.945652   \n",
       "18  {'imputer__strategy': 'median', 'imputer__add_...           0.945652   \n",
       "5   {'imputer__strategy': 'most_frequent', 'impute...           0.945652   \n",
       "14  {'imputer__strategy': 'median', 'imputer__add_...           0.956522   \n",
       "9   {'imputer__strategy': 'median', 'imputer__add_...           0.923913   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "10           0.956044           0.967033           0.956044   \n",
       "18           0.956044           0.978022           0.967033   \n",
       "5            0.956044           0.967033           0.967033   \n",
       "14           0.945055           0.967033           0.956044   \n",
       "9            0.956044           0.967033           0.978022   \n",
       "\n",
       "    split4_test_score  val_score  std_test_score  rank_test_score  \n",
       "10           1.000000   0.964955        0.018782                1  \n",
       "18           0.967033   0.962757        0.011020                2  \n",
       "5            0.967033   0.960559        0.008583                3  \n",
       "14           0.978022   0.960535        0.011171                4  \n",
       "9            0.967033   0.958409        0.018596                5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history.drop('Unnamed: 0', axis=1).sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8340ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [\"SimpleImputer(strategy='median')\",\n",
       "  \"RandomForestClassifier(criterion='entropy')\"]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5cfddc",
   "metadata": {},
   "source": [
    "The classifier can be explained without further ado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c1594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaTabRa] ### Explanation started at 2023-02-09 09:40:19.095028\n",
      "[CaTabRa] *** Split train\n",
      "Sample batches: 100%|########################################| 15/15 [00:00<00:00, 275.13it/s]\n",
      "[CaTabRa] *** Split not_train\n",
      "Sample batches: 100%|########################################| 4/4 [00:00<00:00, 152.09it/s]\n",
      "[CaTabRa] ### Explanation finished at 2023-02-09 09:40:21.726130\n",
      "[CaTabRa] ### Elapsed time: 0 days 00:00:02.631102\n",
      "[CaTabRa] ### Output saved in /mnt/c/Users/amaletzk/Documents/CaTabRa/catabra/examples/random_search_example/explain\n"
     ]
    }
   ],
   "source": [
    "from catabra.explanation import explain\n",
    "\n",
    "explain(\n",
    "    X,\n",
    "    folder='random_search_example',\n",
    "    from_invocation='random_search_example/invocation.json',\n",
    "    out='random_search_example/explain'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c50de",
   "metadata": {},
   "source": [
    "## Extend Existing Auto-Sklearn Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a07cc6",
   "metadata": {},
   "source": [
    "The existing auto-sklearn backend can be easily extended with new components, for instance, for data preprocessing, feature engineering, and predictive modeling. This is independent of CaTabRa and [documented on the official auto-sklearn website](https://automl.github.io/auto-sklearn/master/extending.html), with [examples](https://automl.github.io/auto-sklearn/master/examples/index.html#extension-examples). Additionally, you can check out [`catabra.automl.askl.addons.xgb`](https://github.com/risc-mi/catabra/tree/main/catabra/automl/askl/addons/xgb.py) for details about how CaTabRa adds XGBoost classifiers and regressors to auto-sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
