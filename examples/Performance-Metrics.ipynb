{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea84c89",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de73ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495f93e",
   "metadata": {},
   "source": [
    "This notebook is part of https://github.com/risc-mi/catabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ceb62c",
   "metadata": {},
   "source": [
    "This short example demonstrates how to change the hyperparameter training objective and the metrics reported during training. We focus on binary classification here, but everything applies equally to multiclass- and multilabel classification, and regression.\n",
    "\n",
    "Familiarity with CaTabRa's main data analysis workflow is assumed. A step-by-step introduction can be found in [Workflow.ipynb](https://github.com/risc-mi/catabra/examples/Workflow.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e017b1e",
   "metadata": {},
   "source": [
    "## Inspect Default Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65027a0a",
   "metadata": {},
   "source": [
    "For each of the prediction tasks supported by CaTabRa, a default metric is optimized during hyperparameter tuning. In the case of binary classification this is ROC-AUC, the area under the [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve, as can be seen when inspecting `catabra.core.config.DEFAULT_CONFIG`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52826f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automl': 'auto-sklearn',\n",
       " 'ensemble_size': 10,\n",
       " 'ensemble_nbest': 10,\n",
       " 'memory_limit': 3072,\n",
       " 'time_limit': 1,\n",
       " 'jobs': 1,\n",
       " 'copy_analysis_data': False,\n",
       " 'copy_evaluation_data': False,\n",
       " 'static_plots': True,\n",
       " 'interactive_plots': False,\n",
       " 'bootstrapping_repetitions': 0,\n",
       " 'explainer': 'shap',\n",
       " 'binary_classification_metrics': ['roc_auc', 'accuracy', 'balanced_accuracy'],\n",
       " 'multiclass_classification_metrics': ['accuracy', 'balanced_accuracy'],\n",
       " 'multilabel_classification_metrics': ['f1_macro'],\n",
       " 'regression_metrics': ['r2', 'mean_absolute_error', 'mean_squared_error'],\n",
       " 'ood_class': 'autoencoder',\n",
       " 'ood_source': 'internal',\n",
       " 'ood_kwargs': {},\n",
       " 'auto-sklearn_include': None,\n",
       " 'auto-sklearn_exclude': None,\n",
       " 'auto-sklearn_resampling_strategy': None,\n",
       " 'auto-sklearn_resampling_strategy_arguments': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catabra.core import config\n",
    "config.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27659327",
   "metadata": {},
   "source": [
    "The binary classification metrics are listed under `\"binary_classification_metrics\"`. The first entry in the list is the hyperparameter optimization objective, the remaining entries are additional metrics reported during model training. Likewise, `\"multiclass_classification_metrics\"`, `\"multilabel_classification_metrics\"` and `\"regression_metrics\"` contain the same information for the other prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d43a94",
   "metadata": {},
   "source": [
    "**NOTE**<br>\n",
    "For more information about the possible config parameters and their meaning, please refer to [config.md](https://github.com/risc-mi/catabra/doc/config.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21191b3c",
   "metadata": {},
   "source": [
    "## Change Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4752be",
   "metadata": {},
   "source": [
    "Changing the optimization objective and/or list of metrics reported during model training is easy: simply update the config dict when calling `catabra.analysis.analyze()`, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e02fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eead6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target labels to DataFrame\n",
    "X['diagnosis'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1382b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train- and test set by adding column with corresponding values\n",
    "# the name of the column is arbitrary; CaTabRa tries to \"guess\" which samples belong to which set based on the column name and -values\n",
    "X['train'] = X.index <= 0.8 * len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08659f3",
   "metadata": {},
   "source": [
    "Keyword argument `config` of function `analyze()` allows to update the default config dict. In this example, we use it to specify different binary classification metrics. The value passed to `config` can be either a dict, or the path to a JSON file containing such a dict. The latter is especially useful on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4818b046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaTabRa] ### Analysis started at 2023-02-07 12:50:54.424329\n",
      "[CaTabRa] Saving descriptive statistics completed\n",
      "[CaTabRa] Using AutoML-backend auto-sklearn for binary_classification\n",
      "[CaTabRa] Successfully loaded the following auto-sklearn add-on module(s): xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaletzk/miniconda3/envs/catabra/lib/python3.9/site-packages/autosklearn/metalearning/metalearning/meta_base.py:68: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.metafeatures = self.metafeatures.append(metafeatures)\n",
      "/home/amaletzk/miniconda3/envs/catabra/lib/python3.9/site-packages/autosklearn/metalearning/metalearning/meta_base.py:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.algorithm_runs[metric].append(runs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.937143\n",
      "    n_constituent_models: 1\n",
      "    total_elapsed_time: 00:04\n",
      "[CaTabRa] New model #1 trained:\n",
      "    val_f1: 0.937143\n",
      "    val_sensitivity: 0.921348\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 1.000000\n",
      "    type: random_forest\n",
      "    total_elapsed_time: 00:04\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 2\n",
      "    total_elapsed_time: 00:05\n",
      "[CaTabRa] New model #2 trained:\n",
      "    val_f1: 0.961326\n",
      "    val_sensitivity: 0.977528\n",
      "    val_specificity: 0.919355\n",
      "    train_f1: 0.983607\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:05\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.961326\n",
      "    n_constituent_models: 2\n",
      "    total_elapsed_time: 00:07\n",
      "[CaTabRa] New model #3 trained:\n",
      "    val_f1: 0.937143\n",
      "    val_sensitivity: 0.921348\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 0.989011\n",
      "    type: random_forest\n",
      "    total_elapsed_time: 00:07\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.961326\n",
      "    n_constituent_models: 3\n",
      "    total_elapsed_time: 00:08\n",
      "[CaTabRa] New model #4 trained:\n",
      "    val_f1: 0.931034\n",
      "    val_sensitivity: 0.910112\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 0.989011\n",
      "    type: random_forest\n",
      "    total_elapsed_time: 00:08\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 3\n",
      "    total_elapsed_time: 00:10\n",
      "[CaTabRa] New model #5 trained:\n",
      "    val_f1: 0.935673\n",
      "    val_sensitivity: 0.898876\n",
      "    val_specificity: 0.967742\n",
      "    train_f1: 0.991690\n",
      "    type: extra_trees\n",
      "    total_elapsed_time: 00:09\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 3\n",
      "    total_elapsed_time: 00:11\n",
      "[CaTabRa] New model #6 trained:\n",
      "    val_f1: 0.943182\n",
      "    val_sensitivity: 0.932584\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 1.000000\n",
      "    type: gradient_boosting\n",
      "    total_elapsed_time: 00:10\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 3\n",
      "    total_elapsed_time: 00:12\n",
      "[CaTabRa] New model #7 trained:\n",
      "    val_f1: 0.948571\n",
      "    val_sensitivity: 0.932584\n",
      "    val_specificity: 0.951613\n",
      "    train_f1: 0.983516\n",
      "    type: extra_trees\n",
      "    total_elapsed_time: 00:12\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 3\n",
      "    total_elapsed_time: 00:13\n",
      "[CaTabRa] New model #8 trained:\n",
      "    val_f1: 0.955056\n",
      "    val_sensitivity: 0.955056\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 1.000000\n",
      "    type: gradient_boosting\n",
      "    total_elapsed_time: 00:13\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 5\n",
      "    total_elapsed_time: 00:14\n",
      "[CaTabRa] New model #9 trained:\n",
      "    val_f1: 0.960894\n",
      "    val_sensitivity: 0.966292\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 0.978142\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:14\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.966667\n",
      "    n_constituent_models: 5\n",
      "    total_elapsed_time: 00:16\n",
      "[CaTabRa] New model #10 trained:\n",
      "    val_f1: 0.931818\n",
      "    val_sensitivity: 0.921348\n",
      "    val_specificity: 0.919355\n",
      "    train_f1: 1.000000\n",
      "    type: random_forest\n",
      "    total_elapsed_time: 00:16\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.977778\n",
      "    n_constituent_models: 5\n",
      "    total_elapsed_time: 00:17\n",
      "[CaTabRa] New model #11 trained:\n",
      "    val_f1: 0.966667\n",
      "    val_sensitivity: 0.977528\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 1.000000\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:17\n",
      "[CaTabRa] New model #12 trained:\n",
      "    val_f1: 0.927374\n",
      "    val_sensitivity: 0.932584\n",
      "    val_specificity: 0.887097\n",
      "    train_f1: 1.000000\n",
      "    type: gradient_boosting\n",
      "    total_elapsed_time: 00:18\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.983240\n",
      "    n_constituent_models: 6\n",
      "    total_elapsed_time: 00:21\n",
      "[CaTabRa] New model #13 trained:\n",
      "    val_f1: 0.937143\n",
      "    val_sensitivity: 0.921348\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 0.997230\n",
      "    type: extra_trees\n",
      "    total_elapsed_time: 00:21\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.983240\n",
      "    n_constituent_models: 6\n",
      "    total_elapsed_time: 00:22\n",
      "[CaTabRa] New model #14 trained:\n",
      "    val_f1: 0.956044\n",
      "    val_sensitivity: 0.977528\n",
      "    val_specificity: 0.903226\n",
      "    train_f1: 0.991736\n",
      "    type: lda\n",
      "    total_elapsed_time: 00:21\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.977778\n",
      "    n_constituent_models: 8\n",
      "    total_elapsed_time: 00:23\n",
      "[CaTabRa] New model #15 trained:\n",
      "    val_f1: 0.961326\n",
      "    val_sensitivity: 0.977528\n",
      "    val_specificity: 0.919355\n",
      "    train_f1: 0.991781\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:22\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.977778\n",
      "    n_constituent_models: 8\n",
      "    total_elapsed_time: 00:23\n",
      "[CaTabRa] New model #16 trained:\n",
      "    val_f1: 0.945055\n",
      "    val_sensitivity: 0.966292\n",
      "    val_specificity: 0.887097\n",
      "    train_f1: 0.978022\n",
      "    type: sgd\n",
      "    total_elapsed_time: 00:23\n",
      "[CaTabRa] New model #17 trained:\n",
      "    val_f1: 0.934066\n",
      "    val_sensitivity: 0.955056\n",
      "    val_specificity: 0.870968\n",
      "    train_f1: 1.000000\n",
      "    type: adaboost\n",
      "    total_elapsed_time: 00:25\n",
      "[CaTabRa] New model #18 trained:\n",
      "    val_f1: 0.926554\n",
      "    val_sensitivity: 0.921348\n",
      "    val_specificity: 0.903226\n",
      "    train_f1: 1.000000\n",
      "    type: adaboost\n",
      "    total_elapsed_time: 00:26\n",
      "[CaTabRa] New model #19 trained:\n",
      "    val_f1: 0.937143\n",
      "    val_sensitivity: 0.921348\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 0.997245\n",
      "    type: random_forest\n",
      "    total_elapsed_time: 00:27\n",
      "[CaTabRa] New model #20 trained:\n",
      "    val_f1: 0.898876\n",
      "    val_sensitivity: 0.898876\n",
      "    val_specificity: 0.854839\n",
      "    train_f1: 1.000000\n",
      "    type: k_nearest_neighbors\n",
      "    total_elapsed_time: 00:28\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.977778\n",
      "    n_constituent_models: 7\n",
      "    total_elapsed_time: 00:29\n",
      "[CaTabRa] New model #21 trained:\n",
      "    val_f1: 0.966667\n",
      "    val_sensitivity: 0.977528\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 1.000000\n",
      "    type: gradient_boosting\n",
      "    total_elapsed_time: 00:29\n",
      "[CaTabRa] New model #22 trained:\n",
      "    val_f1: 0.925714\n",
      "    val_sensitivity: 0.910112\n",
      "    val_specificity: 0.919355\n",
      "    train_f1: 0.997245\n",
      "    type: random_forest\n",
      "    total_elapsed_time: 00:30\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.977778\n",
      "    n_constituent_models: 5\n",
      "    total_elapsed_time: 00:33\n",
      "[CaTabRa] New model #23 trained:\n",
      "    val_f1: 0.960894\n",
      "    val_sensitivity: 0.966292\n",
      "    val_specificity: 0.935484\n",
      "    train_f1: 0.994444\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:33\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.983240\n",
      "    n_constituent_models: 6\n",
      "    total_elapsed_time: 00:34\n",
      "[CaTabRa] New model #24 trained:\n",
      "    val_f1: 0.949721\n",
      "    val_sensitivity: 0.955056\n",
      "    val_specificity: 0.919355\n",
      "    train_f1: 1.000000\n",
      "    type: gradient_boosting\n",
      "    total_elapsed_time: 00:34\n",
      "[CaTabRa] New model #25 trained:\n",
      "    val_f1: 0.741667\n",
      "    val_sensitivity: 1.000000\n",
      "    val_specificity: 0.000000\n",
      "    train_f1: 0.744856\n",
      "    type: bernoulli_nb\n",
      "    total_elapsed_time: 00:37\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.983425\n",
      "    n_constituent_models: 7\n",
      "    total_elapsed_time: 00:40\n",
      "[CaTabRa] New model #26 trained:\n",
      "    val_f1: 0.971751\n",
      "    val_sensitivity: 0.966292\n",
      "    val_specificity: 0.967742\n",
      "    train_f1: 1.000000\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:40\n",
      "[CaTabRa] New model #27 trained:\n",
      "    val_f1: 0.913295\n",
      "    val_sensitivity: 0.887640\n",
      "    val_specificity: 0.919355\n",
      "    train_f1: 0.963989\n",
      "    type: adaboost\n",
      "    total_elapsed_time: 00:41\n",
      "[CaTabRa] New ensemble fitted:\n",
      "    ensemble_val_f1: 0.988889\n",
      "    n_constituent_models: 5\n",
      "    total_elapsed_time: 00:45\n",
      "[CaTabRa] New model #28 trained:\n",
      "    val_f1: 0.950276\n",
      "    val_sensitivity: 0.966292\n",
      "    val_specificity: 0.903226\n",
      "    train_f1: 0.975342\n",
      "    type: passive_aggressive\n",
      "    total_elapsed_time: 00:44\n",
      "[CaTabRa] New model #29 trained:\n",
      "    val_f1: 0.106383\n",
      "    val_sensitivity: 0.056180\n",
      "    val_specificity: 1.000000\n",
      "    train_f1: 0.021858\n",
      "    type: bernoulli_nb\n",
      "    total_elapsed_time: 00:45\n",
      "[CaTabRa] New model #30 trained:\n",
      "    val_f1: 0.930481\n",
      "    val_sensitivity: 0.977528\n",
      "    val_specificity: 0.822581\n",
      "    train_f1: 0.949333\n",
      "    type: lda\n",
      "    total_elapsed_time: 00:48\n",
      "[CaTabRa] New model #31 trained:\n",
      "    val_f1: 0.908108\n",
      "    val_sensitivity: 0.943820\n",
      "    val_specificity: 0.806452\n",
      "    train_f1: 0.924675\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaTabRa] New model #32 trained:\n",
      "    val_f1: 0.741667\n",
      "    val_sensitivity: 1.000000\n",
      "    val_specificity: 0.000000\n",
      "    train_f1: 0.744856\n",
      "    type: mlp\n",
      "    total_elapsed_time: 00:50\n",
      "[CaTabRa] Final training statistics:\n",
      "    n_models_trained: 32\n",
      "    ensemble_val_f1: 0.9888888888888888\n",
      "[CaTabRa] Creating shap explainer\n",
      "[CaTabRa] Initialized out-of-distribution detector of type Autoencoder\n",
      "[CaTabRa] Fitting out-of-distribution detector...\n",
      "Iteration 1, loss = 0.06674697\n",
      "Iteration 2, loss = 0.03886039\n",
      "Iteration 3, loss = 0.02630481\n",
      "Iteration 4, loss = 0.01931956\n",
      "Iteration 5, loss = 0.01464805\n",
      "Iteration 6, loss = 0.01285085\n",
      "Iteration 7, loss = 0.01249570\n",
      "Iteration 8, loss = 0.01238648\n",
      "Iteration 9, loss = 0.01221173\n",
      "Iteration 10, loss = 0.01181073\n",
      "Iteration 11, loss = 0.01156576\n",
      "Iteration 12, loss = 0.01151248\n",
      "Iteration 13, loss = 0.01146056\n",
      "Iteration 14, loss = 0.01140084\n",
      "Iteration 15, loss = 0.01138180\n",
      "Iteration 16, loss = 0.01134451\n",
      "Iteration 17, loss = 0.01131035\n",
      "Iteration 18, loss = 0.01130526\n",
      "Iteration 19, loss = 0.01126944\n",
      "Iteration 20, loss = 0.01126597\n",
      "Iteration 21, loss = 0.01125684\n",
      "Iteration 22, loss = 0.01123151\n",
      "Iteration 23, loss = 0.01136320\n",
      "Iteration 24, loss = 0.01122500\n",
      "Iteration 25, loss = 0.01140600\n",
      "Iteration 26, loss = 0.01130277\n",
      "Iteration 27, loss = 0.01126492\n",
      "Iteration 28, loss = 0.01132336\n",
      "Iteration 29, loss = 0.01131247\n",
      "Iteration 30, loss = 0.01122559\n",
      "Iteration 31, loss = 0.01131992\n",
      "Iteration 32, loss = 0.01125866\n",
      "Iteration 33, loss = 0.01126349\n",
      "Iteration 34, loss = 0.01127804\n",
      "Iteration 35, loss = 0.01125354\n",
      "Iteration 36, loss = 0.01124170\n",
      "Iteration 37, loss = 0.01121357\n",
      "Iteration 38, loss = 0.01128867\n",
      "Iteration 39, loss = 0.01122808\n",
      "Iteration 40, loss = 0.01121827\n",
      "Iteration 41, loss = 0.01123449\n",
      "Iteration 42, loss = 0.01121298\n",
      "Iteration 43, loss = 0.01122342\n",
      "Iteration 44, loss = 0.01122471\n",
      "Iteration 45, loss = 0.01120527\n",
      "Iteration 46, loss = 0.01133284\n",
      "Iteration 47, loss = 0.01130642\n",
      "Iteration 48, loss = 0.01128560\n",
      "Iteration 49, loss = 0.01135006\n",
      "Iteration 50, loss = 0.01132227\n",
      "Iteration 51, loss = 0.01128201\n",
      "Iteration 52, loss = 0.01125684\n",
      "Iteration 53, loss = 0.01125339\n",
      "Iteration 54, loss = 0.01121753\n",
      "Iteration 55, loss = 0.01135285\n",
      "Iteration 56, loss = 0.01131737\n",
      "Iteration 57, loss = 0.01125638\n",
      "Iteration 58, loss = 0.01130594\n",
      "Iteration 59, loss = 0.01125258\n",
      "Iteration 60, loss = 0.01121187\n",
      "Iteration 61, loss = 0.01127508\n",
      "Iteration 62, loss = 0.01121970\n",
      "Training loss did not improve more than tol=0.000100 for 50 consecutive epochs. Stopping.\n",
      "[CaTabRa] Out-of-distribution detector fitted.\n",
      "[CaTabRa] ### Analysis finished at 2023-02-07 12:51:58.779368\n",
      "[CaTabRa] ### Elapsed time: 0 days 00:01:04.355039\n",
      "[CaTabRa] ### Output saved in /mnt/c/Users/amaletzk/Documents/CaTabRa/catabra/examples/performance_metrics\n",
      "[CaTabRa] ### Evaluation started at 2023-02-07 12:51:58.826301\n",
      "[CaTabRa] Predicting out-of-distribution samples.\n",
      "[CaTabRa] Saving descriptive statistics completed\n",
      "[CaTabRa] Saving descriptive statistics completed\n",
      "[CaTabRa] Evaluation results for train:\n",
      "    f1 @ 0.5: 0.994475138121547\n",
      "    sensitivity @ 0.5: 1.0\n",
      "    specificity @ 0.5: 0.9838709677419355\n",
      "[CaTabRa] Evaluation results for not_train:\n",
      "    f1 @ 0.5: 0.9942196531791907\n",
      "    sensitivity @ 0.5: 0.9885057471264368\n",
      "    specificity @ 0.5: 1.0\n",
      "[CaTabRa] ### Evaluation finished at 2023-02-07 12:52:04.051203\n",
      "[CaTabRa] ### Elapsed time: 0 days 00:00:05.224902\n",
      "[CaTabRa] ### Output saved in /mnt/c/Users/amaletzk/Documents/CaTabRa/catabra/examples/performance_metrics/eval\n"
     ]
    }
   ],
   "source": [
    "from catabra.analysis import analyze\n",
    "\n",
    "analyze(\n",
    "    X,\n",
    "    classify='diagnosis',     # name of column containing classification target\n",
    "    split='train',            # name of column containing information about the train-test split (optional)\n",
    "    time=1,                   # time budget for hyperparameter tuning, in minutes (optional)\n",
    "    out='performance_metrics',\n",
    "    config={\n",
    "        'binary_classification_metrics': ['f1', 'sensitivity', 'specificity']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b996629",
   "metadata": {},
   "source": [
    "Note that the F1-score, sensitivity and specificity are now reported during model training. The F1-score is the hyperparameter optimization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1265c36",
   "metadata": {},
   "source": [
    "**NOTE**<br>\n",
    "Regardless of the metrics specified in the config dict, evaluating a model with function `catabra.evaluation.evaluate()` always reports *all* suitable built-in performance metrics in `metrics.xlsx`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d802a",
   "metadata": {},
   "source": [
    "## Available Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf7df4",
   "metadata": {},
   "source": [
    "Check out [metrics.md](https://github.com/risc-mi/catabra/doc/metrics.md) for an overview of all built-in metrics available in CaTabRa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
